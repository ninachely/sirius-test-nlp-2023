{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python prepare_messages.py --tg-history-path \"data/result.json\" --output-path \"data/data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /Users/ninachely/.cache/huggingface/datasets/csv/default-d515fd3fd78f0310/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a147f9a05d84ec48569e4d6bb655432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f26109f4cae54e4fb1b57a91e45e6d5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05a1fad7ca0244ec9d9051eaec160565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /Users/ninachely/.cache/huggingface/datasets/csv/default-d515fd3fd78f0310/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"data/data.csv\"\n",
    "\n",
    "data = load_dataset(\"csv\", data_files=DATA_PATH, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['context_3', 'context_2', 'context_1', 'response'],\n",
      "    num_rows: 121829\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc25ad8f33354f8f849b85925233265c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/121829 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['context_3', 'context_2', 'context_1', 'response'],\n",
      "    num_rows: 79105\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "data = data.filter(lambda example: example[\"context_1\"] != None)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['context_3', 'context_2', 'context_1', 'response'],\n",
      "    num_rows: 79105\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context_3': None,\n",
       " 'context_2': None,\n",
       " 'context_1': 'норм цвет)',\n",
       " 'response': 'Грустно видеть Диану с ником куратора\\nто есть ты больше не наш куратор?(('}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.train_test_split(test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context_3': ['Много чего на самом деле)\\nНо меня именно от microsoft штуки интересуют, остальное от физтеховской есть (она гугловская)'],\n",
       " 'context_2': ['Может быть будут работать студенческие подписки'],\n",
       " 'context_1': ['Jetbrains же одной кнопкой продлевается\\nБез всяких подтверждений статуса студента'],\n",
       " 'response': ['Никто не в курсе, когда там приказы-то вообще появятся по коммерции?']}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIRST_SPEAKER_TOKEN = '@@ПЕРВЫЙ@@'\n",
    "SECOND_SPEAKER_TOKEN = '@@ВТОРОЙ@@'\n",
    "\n",
    "CONTEXT_COLS = ['context_3', 'context_2', 'context_1']\n",
    "RESPONSE_COL = ['response']\n",
    "SEP = ' '\n",
    "\n",
    "def convert_to_dialog(sample: Dict[str, str]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "        Convert sample row to dialogs str format\n",
    "    \"\"\"\n",
    "    result_dict = dict()\n",
    "    dialog = \"\"\n",
    "    for i in range(len(CONTEXT_COLS)):\n",
    "        key = CONTEXT_COLS[i]\n",
    "        if key in sample and sample[key] is not None:\n",
    "            speaker_token = FIRST_SPEAKER_TOKEN if i % 2 == 0 else SECOND_SPEAKER_TOKEN\n",
    "            dialog += speaker_token + SEP + sample[key] + SEP\n",
    "    \n",
    "    response_key = RESPONSE_COL[0]\n",
    "    if response_key in sample and sample[response_key] is not None:\n",
    "        dialog += SECOND_SPEAKER_TOKEN + SEP + sample[response_key]\n",
    "\n",
    "    result_dict['text'] = dialog\n",
    "\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '@@ПЕРВЫЙ@@ привет @@ВТОРОЙ@@ привет! @@ПЕРВЫЙ@@ как дела? @@ВТОРОЙ@@ супер)'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_to_dialog(\n",
    "    {\n",
    "        'context_3': 'привет',\n",
    "        'context_2': 'привет!',\n",
    "        'context_1': 'как дела?',\n",
    "        'response': 'супер)'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '@@ПЕРВЫЙ@@ как дела? @@ВТОРОЙ@@ супер)'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_to_dialog(\n",
    "    {\n",
    "        'context_1': 'как дела?',\n",
    "        'response': 'супер)'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert convert_to_dialog(\n",
    "    {\n",
    "        'context_3': 'привет',\n",
    "        'context_2': 'привет!',\n",
    "        'context_1': 'как дела?',\n",
    "        'response': 'супер)'\n",
    "    }\n",
    ") == {'text': '@@ПЕРВЫЙ@@ привет @@ВТОРОЙ@@ привет! @@ПЕРВЫЙ@@ как дела? @@ВТОРОЙ@@ супер)'}\n",
    "\n",
    "assert convert_to_dialog(\n",
    "    {\n",
    "        'context_1': 'как дела?',\n",
    "        'response': 'супер)'\n",
    "    }\n",
    ") == {'text': '@@ПЕРВЫЙ@@ как дела? @@ВТОРОЙ@@ супер)'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 232a1a54-1dcd-4d13-92a1-b91cacc658a1)')' thrown while requesting HEAD https://huggingface.co/tinkoff-ai/ruDialoGPT-medium/resolve/main/tokenizer_config.json\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('tinkoff-ai/ruDialoGPT-medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sample(sample: Dict[str, str]):\n",
    "    return tokenizer(sample['text'], padding='max_length', truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context_3': 'Много чего на самом деле)\\nНо меня именно от microsoft штуки интересуют, остальное от физтеховской есть (она гугловская)', 'context_2': 'Может быть будут работать студенческие подписки', 'context_1': 'Jetbrains же одной кнопкой продлевается\\nБез всяких подтверждений статуса студента', 'response': 'Никто не в курсе, когда там приказы-то вообще появятся по коммерции?'}\n"
     ]
    }
   ],
   "source": [
    "print(data['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55d6c10c32734d3b83b2553d3ea835e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/63284 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41bde37fe4e64765919fe838168f4325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = data['train'].map(convert_to_dialog)\n",
    "test = data['test'].map(convert_to_dialog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6528b429cad24a24a0d1a45de4a53c27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/63284 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "946976b7b0424e3ba2f1a80a634b09c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/63284 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = train.map(tokenize_sample)\n",
    "test = train.map(tokenize_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = AutoModelForCausalLM.from_pretrained('tinkoff-ai/ruDialoGPT-medium').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = {\n",
    "    'output_dir': './training_output',  # path to save the model's checkpoints\n",
    "    'per_device_train_batch_size': 16,  # batch size per GPU/CPU for training\n",
    "    'gradient_accumulation_steps': 4,  # number of batches to accumulate gradient\n",
    "    'max_steps': 500,  # total number of optimizer.step() calls\n",
    "    'save_steps': 100,  # save every save_steps\n",
    "    'eval_steps': 100,  # run evaluation every eval_steps\n",
    "    'dataloader_num_workers': 0,  # number of workers for data loading (default: 0)\n",
    "    'save_total_limit': 2,  # total number of checkpoints to save, delete older checkpoints when reached\n",
    "}\n",
    "\n",
    "trainer = ... # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelWithLMHead\n",
    "\n",
    "checkpoint_path = 'path/to/your/checkpoint-100'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "model = AutoModelWithLMHead.from_pretrained(checkpoint_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
