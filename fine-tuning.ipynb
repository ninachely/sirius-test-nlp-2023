{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python prepare_messages.py --tg-history-path \"data/result.json\" --output-path \"data/data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data/data.csv\"\n",
    "\n",
    "data = load_dataset(\"csv\", data_files=DATA_PATH, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.filter(lambda example: example[\"context_1\"] != None)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.train_test_split(test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['train'][0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIRST_SPEAKER_TOKEN = '@@ПЕРВЫЙ@@'\n",
    "SECOND_SPEAKER_TOKEN = '@@ВТОРОЙ@@'\n",
    "\n",
    "CONTEXT_COLS = ['context_3', 'context_2', 'context_1']\n",
    "RESPONSE_COL = ['response']\n",
    "SEP = ' '\n",
    "\n",
    "def convert_to_dialog(sample: Dict[str, str]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "        Convert sample row to dialogs str format\n",
    "    \"\"\"\n",
    "    result_dict = dict()\n",
    "    dialog = \"\"\n",
    "    for i in range(len(CONTEXT_COLS)):\n",
    "        key = CONTEXT_COLS[i]\n",
    "        if key in sample and sample[key] is not None:\n",
    "            speaker_token = FIRST_SPEAKER_TOKEN if i % 2 == 0 else SECOND_SPEAKER_TOKEN\n",
    "            dialog += speaker_token + SEP + sample[key] + SEP\n",
    "    \n",
    "    response_key = RESPONSE_COL[0]\n",
    "    if response_key in sample and sample[response_key] is not None:\n",
    "        dialog += SECOND_SPEAKER_TOKEN + SEP + sample[response_key]\n",
    "\n",
    "    result_dict['text'] = dialog\n",
    "\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_dialog(\n",
    "    {\n",
    "        'context_3': 'привет',\n",
    "        'context_2': 'привет!',\n",
    "        'context_1': 'как дела?',\n",
    "        'response': 'супер)'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_dialog(\n",
    "    {\n",
    "        'context_1': 'как дела?',\n",
    "        'response': 'супер)'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert convert_to_dialog(\n",
    "    {\n",
    "        'context_3': 'привет',\n",
    "        'context_2': 'привет!',\n",
    "        'context_1': 'как дела?',\n",
    "        'response': 'супер)'\n",
    "    }\n",
    ") == {'text': '@@ПЕРВЫЙ@@ привет @@ВТОРОЙ@@ привет! @@ПЕРВЫЙ@@ как дела? @@ВТОРОЙ@@ супер)'}\n",
    "\n",
    "assert convert_to_dialog(\n",
    "    {\n",
    "        'context_1': 'как дела?',\n",
    "        'response': 'супер)'\n",
    "    }\n",
    ") == {'text': '@@ПЕРВЫЙ@@ как дела? @@ВТОРОЙ@@ супер)'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('tinkoff-ai/ruDialoGPT-medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sample(sample: Dict[str, str]):\n",
    "    return tokenizer(sample['text'], padding='max_length', truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data['train'].map(convert_to_dialog)\n",
    "test = data['test'].map(convert_to_dialog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = train.map(tokenize_sample)\n",
    "tokenized_test = train.map(tokenize_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = AutoModelForCausalLM.from_pretrained('tinkoff-ai/ruDialoGPT-medium').to(device)\n",
    "metric = evaluate.load('f1') # load sth else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = {\n",
    "    'output_dir': './training_output',  # path to save the model's checkpoints\n",
    "    'per_device_train_batch_size': 128,  # batch size per GPU/CPU for training\n",
    "    'gradient_accumulation_steps': 4,  # number of batches to accumulate gradient\n",
    "    'max_steps': 500,  # total number of optimizer.step() calls\n",
    "    'save_steps': 100,  # save every save_steps\n",
    "    'eval_steps': 100,  # run evaluation every eval_steps\n",
    "    'dataloader_num_workers': 0,  # number of workers for data loading (default: 0)\n",
    "    'save_total_limit': 2,  # total number of checkpoints to save, delete older checkpoints when reached\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "\tlogits, labels = eval_pred\n",
    "\tpredictions = np.argmax(logits, axis=-1)\n",
    "\treturn metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "\tmodel = model,\n",
    "\targs = training_args,\n",
    "\ttrain_dataset = tokenized_train,\n",
    "\teval_dataset = tokenized_test,\n",
    "\tcompute_metrics = compute_metrics)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = './pt_save_pretrained'\n",
    "#tokenizer.save_pretrained(save_directory)\n",
    "model.save_pretrained(save_directory)\n",
    "#alternatively save the trainer\n",
    "#trainer.save_model('CustomModels/CustomHamSpam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "# \t'./pt_save_pretrained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelWithLMHead\n",
    "\n",
    "checkpoint_path = 'path/to/your/checkpoint-100'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "model = AutoModelWithLMHead.from_pretrained(checkpoint_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
